# üöÄ Performance Analysis & Optimization Recommendations

## Current Bottlenecks (Ranked by Impact)

### üî¥ CRITICAL: LLM Inference Latency (60-70% of total time)

**Current State:**
- Ollama qwen3:1.7b model running locally
- Temperature=0 (deterministic)
- Each agent makes 1 LLM call = ~2-3 seconds per agent
- 2-3 agents = 4-9 seconds total
- Full product text passed in context window = more tokens to process

**Why it's slow:**
- CPU-bound model (1.7B parameters)
- No quantization (running full fp32 precision)
- Full context window on every call
- Task descriptions are verbose/detailed

**Optimization Ideas:**
1. **Model Quantization** (fastest gain)
   - Use INT8 or INT4 quantization of qwen3:1.7b
   - 4-5x faster inference with minimal quality loss
   - Cost: Small accuracy reduction on edge cases
   - Expected speedup: 4-5x on inference (overall 3-4x)

2. **Reduce Context Size**
   - Product details: send top 3 specs only (not full details)
   - Current: passing 500-1000 chars per product
   - Target: 100-150 chars with key specs
   - Task descriptions: shorten to essentials only
   - Expected speedup: 15-20% (fewer tokens to process)

3. **Use Smaller Model**
   - Switch to qwen2:1.5b or even mistral:7b quantized
   - Would require retraining on banking domain
   - Not recommended without testing
   - Expected speedup: 10-20%

4. **Batch LLM Calls** (if possible with CrewAI)
   - Comparison + Eligibility could theoretically run in parallel
   - CrewAI doesn't support true parallel execution in free tier
   - Would require architectural change
   - Expected speedup: 30-40% (if implementing Comparator + Eligibility parallel)

---

### üü† HIGH: Tool Execution & Vector DB Latency (20-25% of total time)

**Current State:**
- `SearchTools.search_products()` ‚Üí 1 vector DB query
- `SearchTools.get_product_details()` ‚Üí up to 5 queries per product
- Each retriever query can make 3-5 DB calls
- Comparator re-searches if agent decides to look up details
- Eligibility agent also calls get_product_details

**Why it's slow:**
- Multiple sequential DB calls
- Chroma similarity search is not optimized for speed
- Each tool call waits for DB response before next tool call
- Agents call tools unnecessarily even with cached data

**Optimization Ideas:**
1. **Batch Vector DB Queries** (easy win)
   - Instead of: search ‚Üí get_details √ó 3
   - Do: search_batch([product1, product2, product3]) in one call
   - Requires: Chroma batch query wrapper
   - Expected speedup: 40-50% on DB latency (10-12% overall)

2. **Pre-fetch Product Details During Retrieval**
   - Current: Retriever gets product names, other agents fetch details
   - Proposed: Retriever pre-fetches ALL details upfront
   - Store in `SessionState.product_details` dict
   - Agents access dict instead of calling get_product_details
   - Expected speedup: 30-40% on tool calls (8-10% overall)

3. **Reduce Chroma top_k**
   - Current: searching for top_k=5 results
   - Proposed: top_k=3 for retrieval, top_k=1 for get_product_details
   - Tradeoff: Slightly less comprehensive results
   - Expected speedup: 20-30% on DB queries (5-8% overall)

4. **Add Database Caching Layer**
   - Cache product vectors in memory
   - Skip DB query if product name already cached
   - Store: {product_name: {embedding, metadata, details}}
   - Expected speedup: 60-70% on repeat queries (but not first query)

5. **Use Approximate Nearest Neighbor (ANN) Index**
   - Chroma default similarity search is exact
   - Switch to ANN with smaller index
   - Tradeoff: Slightly less accurate but faster
   - Expected speedup: 20-25% on similarity search (3-5% overall)

---

### üü° MEDIUM: Agent Orchestration Overhead (5-10% of total time)

**Current State:**
- CrewAI Crew initialization per kickoff: ~500-800ms
- Agent role/goal/backstory loading
- Task description parsing
- Tool registration
- Sequential agent execution loop

**Why it's slow:**
- Crew object recreated every message
- All agents instantiated even if not used
- Task descriptions loaded into memory

**Optimization Ideas:**
1. **Reuse Crew Objects**
   - Cache agent instances in `BankChatbotCrew.__init__`
   - Recreate Crew only with needed agents
   - Expected speedup: 15-20% on orchestration (1-2% overall)

2. **Parallel Agent Execution** (medium effort)
   - Run Comparator + Eligibility simultaneously
   - CrewAI doesn't support this natively
   - Would require: async TaskGroup or threading
   - Expected speedup: 30-40% if both agents needed (2-4% overall)

3. **Lightweight Task Objects**
   - Replace verbose task descriptions with brief keywords
   - Use structured format instead of dedented prose
   - Expected speedup: 5-10% on task parsing (0.5-1% overall)

4. **Skip Formatter Agent When Not Needed**
   - Feature query doesn't need complex formatting
   - Return raw comparison output if simple query
   - Expected speedup: 20-30% for feature queries (1-2% overall)

---

### üü¢ LOW: App-Level Latency (2-5% of total time)

**Current State:**
- FastAPI request handling: ~50-100ms
- Session management: ~50-100ms
- Intent caching: ~20-50ms
- Response JSON serialization: ~50-100ms

**Why it's slow:**
- Session dict lookups (could use Redis)
- In-memory state management
- JSON serialization of large responses

**Optimization Ideas:**
1. **Redis for Session State**
   - Replace in-memory dict with Redis
   - Distributed session management
   - Only helps at scale (100+ concurrent users)
   - Expected speedup: Not much for single user, 30% for concurrent

2. **Lazy Response Serialization**
   - Build response dict only if needed
   - Avoid serializing unused fields
   - Expected speedup: 5-10% (0.2-0.5% overall)

3. **Early Response Return (Streaming)**
   - Return response headers while LLM is still processing
   - Stream response as chunks
   - Not compatible with current ChatResponse model
   - Expected speedup: Perceived speedup (users see first content faster)

---

## Quick Win Rankings (By Implementation Effort vs Speed Gain)

| Rank | Optimization | Effort | Speedup | Notes |
|------|--------------|--------|---------|-------|
| 1 | Model Quantization | ‚≠ê Low | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê 4-5x | Best ROI, just enable in Ollama |
| 2 | Reduce Context Size | ‚≠ê Low | ‚≠ê‚≠ê 15-20% | Trim product details, task descriptions |
| 3 | Pre-fetch Product Details | ‚≠ê‚≠ê Med | ‚≠ê‚≠ê‚≠ê 8-10% | One code change in retriever |
| 4 | Batch Vector DB Queries | ‚≠ê‚≠ê Med | ‚≠ê‚≠ê‚≠ê 10-12% | Requires Chroma wrapper |
| 5 | Reuse Crew Objects | ‚≠ê‚≠ê Med | ‚≠ê 1-2% | Cache agents, not worth it |
| 6 | Reduce top_k Values | ‚≠ê Low | ‚≠ê‚≠ê 5-8% | Trade accuracy for speed |
| 7 | Parallel Agent Execution | ‚≠ê‚≠ê‚≠ê High | ‚≠ê‚≠ê‚≠ê 2-4% | Complex, low impact |
| 8 | ANN Index in Chroma | ‚≠ê‚≠ê‚≠ê High | ‚≠ê‚≠ê 3-5% | Overkill for small dataset |

---

## Recommended Implementation Path (Priority Order)

### Phase 1: Quick Wins (15 mins)
1. **Enable Ollama Quantization**
   - Modify docker-compose to use `qwen3:1.7b-q4_K_M` quantized model
   - 4-5x faster inference
   - Result: Comparison goes from 2-3s ‚Üí 0.5-0.8s

2. **Trim Context Size**
   - Reduce product details from 500 chars ‚Üí 150 chars
   - Shorten task descriptions (remove examples, keep essentials)
   - Result: 15-20% faster token processing

### Phase 2: Medium Effort (1-2 hours)
3. **Pre-fetch Product Details in Retriever**
   - When retriever finds products, fetch all details immediately
   - Store in SessionState.product_details dict
   - Agents access dict instead of calling tools
   - Result: 8-10% faster overall, zero "TBD" values

4. **Batch Vector DB Queries**
   - Create batch_search() wrapper in SearchTools
   - Query 3 products in parallel instead of sequential
   - Result: 10-12% faster DB latency

### Phase 3: Advanced (3-4 hours, optional)
5. **Parallel Agent Execution**
   - Use asyncio to run Comparator + Eligibility together
   - Requires CrewAI async support investigation
   - Result: 2-4% faster (low priority)

---

## Expected Timeline Results

### After Phase 1 (Quick Wins)
```
Message 1 (Retrieval):     4-5s  (was 8-10s) 
Message 2 (Comparison):    0.5-1s (was 2-3s)  ‚úÖ 3-4x faster!
Message 3 (Eligibility):   0.5-1s (was 2-3s)  ‚úÖ 3-4x faster!
Total:                     5-7s   (was 12-16s) ‚úÖ 60% faster
```

### After Phase 1 + Phase 2
```
Message 1 (Retrieval):     3-4s   (was 8-10s)
Message 2 (Comparison):    0.3-0.5s (was 2-3s) ‚úÖ 5-6x faster!
Message 3 (Eligibility):   0.3-0.5s (was 2-3s) ‚úÖ 5-6x faster!
Total:                     3.5-5s (was 12-16s) ‚úÖ 70% faster
```

---

## What NOT to Optimize

‚ùå **Don't optimize the formatter agent**
- It's only 10% of time
- Formatting quality matters more than speed

‚ùå **Don't add Redis yet**
- Not needed for single-user or small concurrent load
- Adds complexity, minimal gain

‚ùå **Don't switch models**
- Qwen3 1.7B is already optimal for local
- Switching breaks compatibility

‚ùå **Don't implement parallel agents yet**
- CrewAI doesn't support it natively
- Too complex for 2-4% gain

---

## Implementation Impact Summary

**Phase 1 Alone: 60% faster + minimal code changes**
- Just enable quantization in docker-compose
- Trim context in 2-3 files
- 15-20 minutes work

**Phase 1 + 2: 70% faster + moderate code changes**
- Pre-fetch product details
- Batch vector queries
- 2-3 hours work

**Everything: 75% faster (diminishing returns)**
- Phase 3 adds only 2-4% for high effort
- Not recommended unless targeting <2s response

---

## Monitoring to Verify Improvements

Track these metrics after each phase:
```
- Message response time (milliseconds)
- LLM inference time per agent
- Vector DB query latency
- Number of tool calls per agent
- Tokens processed per response
- Context window size
```

Add logging:
```python
# Before agent run
start_time = time.time()

# After agent run
elapsed = time.time() - start_time
print(f"Agent {agent.role} took {elapsed:.2f}s")
```

This gives clear visibility into which bottleneck is actually dominant.
